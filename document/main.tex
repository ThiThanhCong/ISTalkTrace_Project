\documentclass{article}
\usepackage{amsmath, amssymb, graphicx, cite}
\usepackage[T5]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cite}

\title{Lí thuyết IS-Talk-Trance và QCNN}
\author{}
\date{}

\begin{document}

\maketitle

\section{Trích Xuất MFCC}
Quy trình trích xuất MFCC bao gồm nhiều bước tiền xử lý để chuyển đổi tín hiệu giọng nói từ miền thời gian sang miền tần số và sau đó trích xuất các đặc trưng cepstral.

\subsection{Giới thiệu}
Mel-Frequency Cepstral Coefficients (MFCC) là một trong những phương pháp phổ biến nhất để trích xuất đặc trưng giọng nói trong các hệ thống nhận dạng giọng nói tự động (ASR). MFCC mô phỏng cách con người cảm nhận âm thanh bằng cách áp dụng thang đo Mel, giúp làm nổi bật các đặc trưng quan trọng trong tín hiệu giọng nói \cite{davis1980comparison}.

\subsection{Các Bước Trích Xuất MFCC}
\subsubsection{Chuyển đổi A/D}
Tín hiệu giọng nói ban đầu là một tín hiệu tương tự và cần được chuyển đổi sang dạng số bằng cách lấy mẫu (sampling) và lượng tử hóa (quantization) \cite{oppenheim1999discrete}.

\subsubsection{Tiền xử lý (Pre-emphasis)}
Tín hiệu đầu vào được lọc qua bộ lọc high-pass để nhấn mạnh các tần số cao, giúp cân bằng phổ và giảm tác động của tiếng ồn nền.

\subsubsection{Cửa sổ (Windowing)}
Cửa sổ Hamming hoặc Hanning được áp dụng để giảm hiệu ứng rò rỉ phổ (spectral leakage) trước khi thực hiện phân tích tần số.

\subsubsection{Biến đổi Fourier (DFT)}
Biến đổi Fourier rời rạc (DFT) được sử dụng để chuyển tín hiệu từ miền thời gian sang miền tần số, giúp trích xuất thông tin phổ \cite{rabiner1993fundamentals}.

\subsubsection{Bộ lọc Mel (Mel Filterbank)}
Dải tần số được ánh xạ sang thang đo Mel, giúp mô phỏng cách con người nhận thức âm thanh với độ phân giải cao hơn ở tần số thấp.

\subsubsection{Biến đổi Log}
Áp dụng hàm logarit lên phổ Mel để nén dải động và làm cho phổ có đặc tính gần hơn với cách tai người cảm nhận âm thanh.

\subsubsection{Biến đổi nghịch IDFT (Cepstral Analysis)}
DCT (Discrete Cosine Transform) được sử dụng để chuyển phổ log-Mel về miền cepstral, giúp giảm mối tương quan giữa các đặc trưng.

\subsubsection{Trích Xuất Đặc Trưng Động}
Tính đạo hàm bậc nhất ($\Delta$) và bậc hai ($\Delta^2$) của MFCC để mô tả sự thay đổi của đặc trưng theo thời gian, làm tăng độ chính xác của hệ thống nhận dạng giọng nói.

% X-VECTOR
\section{Tổng Quan về X-Vector}

X-Vector được giới thiệu bởi Snyder et al. (2018) như một phương pháp dựa trên mạng nơ-ron sâu để trích xuất đặc trưng giọng nói.
Mô hình này bao gồm một mạng nơ-ron sâu (DNN) được huấn luyện để học biểu diễn đặc trưng từ các đoạn âm thanh với độ dài khác nhau.

\subsection{Kiến Trúc X-Vector}

Mô hình X-Vector được xây dựng dựa trên một mạng DNN có cấu trúc chính gồm các thành phần:

\begin{itemize}
    \item \textbf{Layer tiền xử lý}: Biến đổi đầu vào bằng các bộ lọc để tạo ra biểu diễn đặc trưng cục bộ.
    \item \textbf{Layer frame-level}: Một chuỗi các lớp CNN hoặc TDNN (Time Delay Neural Network) trích xuất đặc trưng từ từng khung âm thanh.
    \item \textbf{Layer thống kê}: Tổng hợp thông tin từ toàn bộ đoạn giọng nói để tạo ra một biểu diễn cố định.
    \item \textbf{Layer speaker embedding}: Mã hóa thông tin giọng nói dưới dạng vector X-Vector có kích thước cố định.
\end{itemize}

\subsection{Quy Trình Huấn Luyện}

Mô hình X-Vector được huấn luyện trên dữ liệu giọng nói lớn, sử dụng chức năng mất mát softmax để phân loại người nói.
Sau khi huấn luyện, các vector đặc trưng được rút trích từ lớp embedding để sử dụng trong các tác vụ khác nhau.
\section{Mạng Tích Chập Lượng Tử (QCNN)}

Mạng tích chập lượng tử (Quantum Convolutional Neural Network - QCNN) là một phiên bản lượng tử của mạng tích chập (CNN), được thiết kế để xử lý dữ liệu trong môi trường lượng tử. QCNN bao gồm các thành phần chính sau:

\begin{itemize}
    \item \textbf{Lớp tích chập lượng tử (Quantum Convolution Layer)}: Trích xuất đặc trưng từ trạng thái lượng tử đầu vào bằng cách áp dụng các cổng lượng tử như cổng Hadamard, cổng Pauli, hoặc cổng kiểm soát (CNOT). Phép tích chập lượng tử được biểu diễn bởi một ma trận unitary $U_{\text{conv}}$:
          \begin{equation}
              U_{\text{conv}} | \psi \rangle = | \phi \rangle
          \end{equation}
          trong đó $|\psi\rangle$ là trạng thái đầu vào và $|\phi\rangle$ là trạng thái sau tích chập.

    \item \textbf{Lớp gộp lượng tử (Quantum Pooling Layer)}: Giúp giảm số lượng qubit, tương tự như max pooling hoặc average pooling trong CNN. Phép gộp lượng tử sử dụng phép đo qubit hoặc các cổng kiểm soát để loại bỏ thông tin không quan trọng.

    \item \textbf{Lớp kết nối đầy đủ lượng tử (Quantum Fully Connected Layer)}: Kết hợp các đặc trưng đã trích xuất để tạo ra đầu ra cuối cùng. Nó được biểu diễn bởi một ma trận unitary $U_{\text{fc}}$:
          \begin{equation}
              U_{\text{fc}} | \psi \rangle = | \psi_{\text{out}} \rangle
          \end{equation}

    \item \textbf{Phép đo lượng tử (Measurement)}: Sau khi xử lý qua các lớp lượng tử, trạng thái lượng tử được chuyển về dữ liệu cổ điển thông qua phép đo xác suất trạng thái:
          \begin{equation}
              P(i) = | \langle i | \psi_{\text{out}} \rangle |^2
          \end{equation}
\end{itemize}

\section{Hidden Markov Model}
HMM là một mô hình xác suất dùng để mô tả một chuỗi các quan sát $O = \{o_1, o_2, ..., o_T\}$ thông qua một tập hợp trạng thái ẩn $Q = \{q_1, q_2, ..., q_N\}$.

Một HMM được định nghĩa bởi ba tham số chính:
\begin{itemize}
    \item $A = [a_{ij}]$: Ma trận xác suất chuyển trạng thái, trong đó $a_{ij} = P(q_{t+1} = j \mid q_t = i)$.
    \item $B = [b_j(o_t)]$: Xác suất quan sát (output probability), trong đó $b_j(o_t) = P(o_t \mid q_t = j)$.
    \item $\pi = [\pi_i]$: Xác suất ban đầu của trạng thái, $\pi_i = P(q_1 = i)$.
\end{itemize}

\subsection{Mô hình HMM cho Nhận dạng Giọng Nói}
Trong ASR, mỗi từ hoặc âm vị có thể được mô hình hóa bằng một HMM riêng lẻ. Khi một chuỗi tín hiệu giọng nói được đưa vào, mục tiêu là tìm ra chuỗi trạng thái $Q^*$ sao cho xác suất quan sát tối đa:
\begin{equation}
    Q^* = \arg\max_Q P(O \mid Q, \lambda)
\end{equation}
trong đó $\lambda = (A, B, \pi)$ là tham số của HMM.

\subsection{Các thuật toán chính}
\subsubsection{Thuật toán Viterbi}
Viterbi là thuật toán quan trọng để tìm chuỗi trạng thái tối ưu nhất dựa trên mô hình HMM. Xác suất tối đa cho trạng thái $j$ tại thời điểm $t$ được tính như sau:
\begin{equation}
    \delta_t(j) = \max_i \left[ \delta_{t-1}(i) a_{ij} \right] b_j(o_t)
\end{equation}
\subsubsection{Thuật toán Baum-Welch}
Để huấn luyện HMM, thuật toán Baum-Welch (một biến thể của Expectation-Maximization) được sử dụng để ước lượng tham số:
\begin{equation}
    a_{ij} = \frac{\sum_{t=1}^{T-1} P(q_t = i, q_{t+1} = j \mid O, \lambda)}{\sum_{t=1}^{T-1} P(q_t = i \mid O, \lambda)}
\end{equation}

\section{Mạng Nơ-ron LSTM và X-vector trong Nhận Dạng Giọng Nói}
LSTM là một biến thể của mạng nơ-ron truy hồi (RNN) giúp khắc phục vấn đề vanishing gradient trong mô hình hóa chuỗi thời gian dài. Công thức cập nhật trạng thái trong LSTM được biểu diễn như sau:
\begin{align}
    f_t         & = \sigma(W_f x_t + U_f h_{t-1} + b_f)       \\
    i_t         & = \sigma(W_i x_t + U_i h_{t-1} + b_i)       \\
    \tilde{C}_t & = \tanh(W_c x_t + U_c h_{t-1} + b_c)        \\
    C_t         & = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
    o_t         & = \sigma(W_o x_t + U_o h_{t-1} + b_o)       \\
    h_t         & = o_t \odot \tanh(C_t)
\end{align}
Trong đó, $f_t, i_t, o_t$ lần lượt là cổng quên, cổng nhập và cổng đầu ra; $C_t$ là trạng thái bộ nhớ; $h_t$ là trạng thái ẩn.

X-vector là phương pháp trích xuất đặc trưng sử dụng mạng DNN/LSTM để mô hình hóa giọng nói \cite{Snyder2018}. Nó bao gồm các lớp FC (Fully Connected) để tạo đặc trưng biểu diễn giọng nói từ cửa sổ tín hiệu âm thanh:
\begin{equation}
    x = \text{FC}_4(\text{FC}_3(\text{FC}_2(\text{FC}_1(O))))
\end{equation}
Trong đó, $O$ là chuỗi quan sát, và các lớp FC giúp trích xuất đặc trưng có độ phân biệt cao.


\begin{thebibliography}{9}
    \bibitem{Snyder2018} D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, "X-vectors: Robust DNN embeddings for speaker recognition," \textit{ICASSP}, pp. 5329-5333, 2018.
    \bibitem{Davis1980} S. B. Davis and P. Mermelstein, \textquotedblleft Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences,\textquotedblright \emph{IEEE Transactions on Acoustics, Speech, and Signal Processing}, vol. 28, no. 4, pp. 357–366, 1980.
    \bibitem{davis1980comparison} S. B. Davis and P. Mermelstein, ``Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences,'' \emph{IEEE Transactions on Acoustics, Speech, and Signal Processing}, vol. 28, no. 4, pp. 357–366, 1980.
    \bibitem{oppenheim1999discrete} A. V. Oppenheim and R. W. Schafer, \emph{Discrete-time signal processing}, Prentice Hall, 1999.
    \bibitem{rabiner1993fundamentals} L. Rabiner and B. H. Juang, \emph{Fundamentals of speech recognition}, Prentice Hall, 1993.
    \bibitem{Rabiner1989} L. R. Rabiner, "A tutorial on hidden Markov models and selected applications in speech recognition," \textit{Proceedings of the IEEE}, vol. 77, no. 2, pp. 257-286, 1989.
    \bibitem{Hinton2012} G. Hinton et al., "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups," \textit{IEEE Signal Processing Magazine}, vol. 29, no. 6, pp. 82-97, 2012.
\end{thebibliography}
\end{document}
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage[T5]{fontenc}
\usepackage[vietnamese,english]{babel}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    \makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother
\begin{document}


\title{BUILDING A REAL-TIME SPEECH EXTRACTION, SUMMARIZATION, AND TRANSLATION SYSTEM THROUGH THE INTEGRATION OF MACHINE LEARNING MODELS\\
}
\author{
    \IEEEauthorblockN{1\textsuperscript{st} Cao Hoài Sang}
    \IEEEauthorblockA{\textit{Department of Information Systems} \\
        \textit{University of Information Technology} \\
        Ho Chi Minh City, Viet Nam \\
        21522541@gm.uit.edu.vn}
    %
    \and
    %
    \IEEEauthorblockN{2\textsuperscript{nd} Thi Thành Công}
    \IEEEauthorblockA{\textit{Department of Information Systems} \\
        \textit{University of Information Technology} \\
        Ho Chi Minh City, Viet Nam \\
        21521897@gm.uit.edu.vn}
    %
    \linebreakand
    \IEEEauthorblockN{3\textsuperscript{rd} Assoc. Prof. Nguyễn Đình Thuân}
    \IEEEauthorblockA{\textit{Department of Information Systems} \\
        \textit{University of Information Technology} \\
        Ho Chi Minh City, Viet Nam \\
        thuannd@hcmuit.edu.vn}
    %
    \and
    \IEEEauthorblockN{4\textsuperscript{th} Nguyễn Minh Nhựt}
    \IEEEauthorblockA{\textit{Department of Information Systems} \\
        \textit{University of Information Technology} \\
        Ho Chi Minh City, Viet Nam \\
        nhutnm.17@grad.uit.edu.vn}
}

\maketitle

\begin{abstract}
    In the age of digital communication and telework, the need for real-time conversation processing becomes increasingly important. This study proposes an integrated system capable of speech separation, speaker recognition, individualized content summarization, and real-time voice translation, while preserving the original voice characteristics. The approach combines acoustic features such as MFCC, XVector and DVector and modern deep learning models such as RNN, GMM and Transformer. The system is trained and tested on the actual labeled conversation dataset. The results show that the system achieves high accuracy in discriminating speakers and summarizing content, while ensuring low latency suitable for real-time application.
\end{abstract}


\section{Introduction}
In the context of increasingly common online meetings and interactions, the need for real-time speech processing and analysis has become urgent. Current speech recognition systems primarily focus on converting speech into text but do not fully support advanced functions such as speaker differentiation, individual conversation summarization, and real-time language translation.

This research aims to develop a real-time speech processing system that integrates multiple functionalities: speaker identification, conversation analysis, and summarization on a per-person basis, while also translating speech into another language without losing the original voice characteristics. The system will apply modern deep learning models in combination with acoustic features to achieve high performance.



\section{Related Work}

In the field of real-time speech recognition and analysis, multiple approaches have been proposed with the aid of machine learning and deep learning techniques. This section presents an overview of the relevant approaches, divided into groups based on the characteristic methodology and the model used.

\subsection{Mel-Frequency Cepstral Coefficients (MFCC)}

MFCC is one of the most common features in speech signal processing. The article on the use of MFCC to extract characteristics for training HMM models (hidden Markov models) by Rabiner~\cite{rabiner1989tutorial} detailed the use of hidden Markov models (HMM) combined with MFCC characteristics for speech recognition.
\subsection{Wavelet-based Features}

Wavelet transform is a powerful tool used to exploit time and frequency information from audio signals. Wavelet shows good ability to handle sound in noisy or unstable environments. Although not as widely available as MFCC, wavelets have recently been applied to systems thanks to their powerful noise cancellation capabilities. Studies have shown that using wavelet transforms in feature extraction can improve the accuracy of speech recognition systems, particularly in environments with noise \cite{gupta2003robust, wang2008robust}.

\subsection{X-vector and D-vector Embeddings}

X-vectors and D-vectors are representations learned from audio data, often used for speaker recognition. X-vector systems often use DNN or TDNN networks to learn performances to improve speaker discrimination accuracy. These representations are widely used in modern processing, for example in the systems of Kaldi and SpeechBrain. Snyder et al.~\cite{Snyder2018Xvectors} introduced X-vector as a deep learning representation for speaker recognition. For D-vector, Wan et al.~\cite{Wan2018Generalized} proposed using the GE2E loss function for training, which improves performance in speaker recognition.

\subsection{Hidden Markov Models (HMM)}

HMM is the foundation of many traditional ASR systems. Rabiner \cite{rabiner1989tutorial} has proposed effective Viterbi training methods and algorithms for HMM in speech recognition. Hybrid systems such as in~\cite{voll2007hybrid, perero2022comparison} also show that combining HMM with neural networks improves accuracy in real-world environments.

\subsection{Recurrent Neural Networks (RNN)}

RNN is a popular model in ASR because it can capture temporal dependencies in speech data. LSTM and GRU variants help solve vanishing gradient problems and are effective in modeling long-term context. Systems like Deep Speech~\cite{hannun2014deep} and work from Graves et al.~\cite{graves2013speech} show that RNN-based models can achieve good results in end-to-end ASR tasks.

\subsection{Quantum Convolutional Neural Networks (QCNN)}

QCNN is a new model in quantum deep learning, proposed to take advantage of quantum properties to reduce the number of redundant parameters, focus on key characteristics, and enhance representability. Cong et al.~\cite{Cong2019QuantumCNN} introduced QCNN as a new quantum machine learning model, which combines with the characteristics of convolutional neural networks and quantum properties to classify quantum states. Although QCNN has not been widely applied in ASR, research by Yang et al.~\cite{Yang2021Decentralizing} has shown potential in combining QCNN with other

\section{Resource}
\subsection{Dataset}
The dataset used for training and evaluating the speech recognition system consists of a series of Vietnamese conversations collected from various sources. These data sources include natural conversations, group discussions, podcasts, lectures, as well as direct interviews. The dataset reflects the diversity of voices, expressions, and speaking speeds of individual speakers, which helps the model learn the phonetic, prosodic, and pronunciation features of speakers in various contexts.

\subsection{Descriptive Statistics}
The total size of the dataset is 15GB, with many measurements listed below:
\begin{table}[h]
    \centering
    \caption{Overview of speech dataset statistics}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Statistic}                  & \textbf{Value}               \\
        \hline
        Total number of speakers            & 83                           \\
        Total number of dialogues           & 10,430                       \\
        Number of speaker changes           & 751                          \\
        Total number of audio files         & 85                           \\
        Total number of script files (.txt) & 85                           \\
        Total duration of dataset           & 64,608 seconds (17.93 hours) \\
        Average dialogue length             & 6.19 seconds                 \\
        Standard deviation of length        & 11.96 seconds                \\
        \hline
    \end{tabular}
\end{table}
\section{Background}
\begin{figure}[H]
    \centering
    \resizebox{0.55\textwidth}{!}{%
        \begin{tikzpicture}[
            node distance=1.0cm,
            every node/.style={font=\small},
            box/.style={draw, rounded corners, minimum width=2.5cm, minimum height=1.3cm, align=center, fill=cyan!20},
            line/.style={draw, thick, -{Stealth}}
            ]

            % Nodes
            \node[fill=blue!10] (input) {Input Speech Signal};
            \node[box, fill=green!10, right=of input] (feature) {Feature Extraction};
            \node[box, fill=orange!10, right=of feature] (model) {Model Processing};
            \node[fill=red!10, right=of model] (output) {Speaker Identity};

            % Arrows
            \draw[line] (input) -- (feature);
            \draw[line] (feature) -- (model);
            \draw[line] (model) -- (output);

        \end{tikzpicture}%
    }
    \label{fig:speaker_identification_pipeline}
    \caption{Speaker Identification Pipeline}
\end{figure}

An ASR system identification speaker. Given an input audio \(x\), the ASR model \(f(\cdot)\) will find the best fit speaker with the feature of the speaker. The typical architecture
of the ASR system is illustrated in figure~\ref{fig:speaker_identification_pipeline}, he first step, \textbf{feature extraction},
captures the acoustic characteristics of the input speech signal. Traditional systems often rely on \textbf{MFCC} to extract time-frequency representations of speech,
while modern systems have adopted more advanced techniques, such as \textbf{XVector}, \textbf{DVector}, and \textbf{Wavelet}, which offer enhanced representations of
speaker traits in different domains. In the next phase, \textbf{model processing}, the extracted features or raw spectrogram are fed into models such as \textbf{Hidden Markov Models (HMM)},
\textbf{Recurrent Neural Networks (RNN)}. These models map the features to an intermediate representation,
enabling accurate speaker identification. \cite{davis1980comparison, Snyder2018Xvectors,wan2018dvector,graves2013speech}.

\subsection{Hidden Markov Model (HMM)}
HMM is a probabilistic model used to describe a sequence of observations $O = \{o_1, o_2, ..., o_T\}$ through a set of hidden states $Q = \{q_1, q_2, ..., q_N\}$.

An HMM is defined by three main parameters:
\begin{itemize}
    \item $A = [a_{ij}]$: The state transition probability matrix, where $a_{ij} = P(q_{t+1} = j \mid q_t = i)$.
    \item $B = [b_j(o_t)]$: The observation (output) probability, where $b_j(o_t) = P(o_t \mid q_t = j)$.
    \item $\pi = [\pi_i]$: The initial state distribution, $\pi_i = P(q_1 = i)$.
\end{itemize}

In ASR, each word or phoneme can be modeled by an individual HMM. When a speech signal sequence is input, the goal is to find the optimal state sequence $Q^*$ that maximizes the observation probability:
\begin{equation}
    Q^* = \arg\max_Q P(O \mid Q, \lambda)
\end{equation}
where $\lambda = (A, B, \pi)$ are the parameters of the HMM.

The Viterbi algorithm is essential for finding the most likely state sequence based on the HMM. The maximum probability for state $j$ at time $t$ is computed as:
\begin{equation}
    \delta_t(j) = \max_i \left[ \delta_{t-1}(i) a_{ij} \right] b_j(o_t)
\end{equation}

To train the HMM, the Baum-Welch algorithm (a variant of Expectation-Maximization) is used to estimate the parameters:
\begin{equation}
    a_{ij} = \frac{\sum_{t=1}^{T-1} P(q_t = i, q_{t+1} = j \mid O, \lambda)}{\sum_{t=1}^{T-1} P(q_t = i \mid O, \lambda)}
\end{equation}

\subsection{Recurrent Neural Networks (RNNs)}

Recurrent Neural Networks (RNNs) are widely used in ASR systems due to their ability to model temporal dependencies in sequential data. Unlike feedforward networks, RNNs maintain a hidden state that captures context from previous time steps, making them suitable for processing variable-length speech signals. Variants such as Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) have been shown to effectively capture long-range dependencies and improve recognition performance \cite{graves2013speech}.




\subsection{Mel Frequency Cepstral Coefficients (MFCCs)}
Mel-Frequency Cepstral Coefficients (MFCC) are widely used in ASR systems to extract speech features by
mimicking human auditory perception. The analog speech signal is first digitized, then passed through a
high-pass filter and windowed to minimize noise and spectral leakage. The Discrete Fourier Transform (DFT)
converts the signal to the frequency domain, which is then mapped to the Mel scale. A logarithmic function
is applied to compress the dynamic range, and the Discrete Cosine Transform (DCT) converts this to the cepstral
domain. Finally, first- and second-order derivatives ($\Delta$, $\Delta^2$) are added to capture temporal dynamics. \cite{davis1980comparison}


\subsection{Wavelet}
Wavelet Transform (WT) is a mathematical tool that provides a time-frequency representation of signals. Unlike the Fourier Transform, which only provides frequency information, the wavelet transform can represent both time and frequency information simultaneously. This makes it especially useful for analyzing non-stationary signals like speech.

The Continuous Wavelet Transform of a signal $x(t)$ is defined as:

\[
    W(a, b) = \int_{-\infty}^{\infty} x(t) \frac{1}{\sqrt{|a|}} \psi\left(\frac{t - b}{a} \right) dt
\]

where:
\begin{itemize}
    \item $a$ is the scale parameter (related to frequency),
    \item $b$ is the translation parameter (related to time),
    \item $\psi(t)$ is the mother wavelet.
\end{itemize}

\paragraph{Discrete Wavelet Transform}
The DWT is a sampled version of the CWT and is more suitable for implementation in digital systems. It decomposes a signal into approximation and detail coefficients using filter banks. The signal is passed through a series of low-pass and high-pass filters followed by downsampling. \cite{tufekci2000dwt}

\paragraph{HMM and Wavelet-based Features}
We can replace MFCCs with the Wavelet coefficients in the observation vectors used in HMM training and decoding.


\subsubsection{Quantum Convolutional Neural Network (QCNN) - Mel Frequency Cepstral Coefficients (MFCCs)}

In this approach, Mel-Frequency Cepstral Coefficients (MFCCs) are used as the input features to a Quantum Convolutional Neural Network (QCNN). MFCCs are effective in capturing the spectral characteristics of speech signals and are widely used in speaker recognition tasks.

\paragraph{MFCC Extraction}

The MFCC extraction pipeline consists of the following steps:

\begin{itemize}
    \item \textbf{Pre-emphasis:} Apply a high-pass filter to amplify high frequencies:
          \[
              H(z) = 1 - \alpha z^{-1}, \quad \alpha \approx 0.95
          \]
    \item \textbf{Framing and Windowing:} Divide the signal into overlapping frames and apply a window function (e.g., Hamming).
    \item \textbf{FFT:} Convert each frame to the frequency domain.
    \item \textbf{Mel Filter Bank:} Apply a bank of filters spaced in the mel scale.
    \item \textbf{Logarithm:} Compute the log energy of each filter output.
    \item \textbf{DCT:} Apply Discrete Cosine Transform to obtain decorrelated MFCCs.
\end{itemize}


\subsection{X-Vector}

The X-Vector model is built upon a deep neural network and typically consists of the following
components:

\begin{itemize}
    \item \textbf{Preprocessing layers}: These layers apply filtering operations to the input features,
          generating localized representations.
    \item \textbf{Frame-level layers}: A series of convolutional (CNN) or time
          delay neural network (TDNN) layers extract features at the frame level.
    \item \textbf{Statistics pooling layer}: Aggregates frame-level outputs
          across the entire utterance using statistical functions (e.g., mean and standard deviation) to produce a fixed-dimensional representation.
    \item \textbf{Speaker embedding layer}: Generates a fixed-length X-Vector
          encoding speaker-specific information.
\end{itemize}

The X-Vector model is trained on large-scale speaker-labeled datasets using a softmax loss function for speaker classification.
After training, speaker embeddings are extracted from the penultimate layer and can be used for downstream tasks such as speaker
verification or clustering.


















\paragraph{Quantum Encoding}

The MFCC vectors are normalized and encoded into a quantum state using angle encoding (specifically \( RY \) rotations):

\begin{equation}
    |\psi\rangle = \bigotimes_{k=1}^{n} RY(x_k) |0\rangle
\end{equation}

\paragraph{QCNN Architecture}

The QCNN consists of alternating layers of quantum convolution and pooling operations followed by a variational layer. The general structure is:

\begin{equation}
    |\phi\rangle = U_{\text{QCNN}} |\psi\rangle = (U_{\text{var}} \cdot U_{\text{pool}} \cdot U_{\text{conv}}) |\psi\rangle
\end{equation}

\begin{itemize}
    \item \textbf{Quantum Convolution:} Apply local entangling unitaries:
          \[
              U^{(i,j)}_{\text{conv}} = RZ_i(\theta) \cdot CX_{ij} \cdot RZ_j(\phi) \cdot CX_{ji} \cdot RZ_i(\lambda)
          \]
    \item \textbf{Quantum Pooling:} Perform controlled unitary operations or partial measurements to reduce the number of active qubits.
    \item \textbf{Variational Layer:} Apply trainable unitaries to remaining qubits for classification.
\end{itemize}

\paragraph{Measurement and Output}

At the end of the QCNN, selected qubits are measured using Pauli-Z expectation:

\begin{equation}
    \langle Z \rangle = \langle \phi | Z | \phi \rangle
\end{equation}

The measured values are used to compute prediction probabilities or classification scores. Training is done by minimizing a loss function such as cross-entropy or MSE using gradient-based optimizers.

\bibliographystyle{IEEEtran}
\bibliography{resource/references}
\end{document}
